{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1wP+Fa8swBtWwhX4jRJMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Timure228/Hands-on-ML/blob/main/chapter_19_training_deploying_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The code below is not to be executed"
      ],
      "metadata": {
        "id": "y2qvQ5jXFOF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDgn3ZAyCG74"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "# Save Models\n",
        "\n",
        "model.save(path, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert input images from NumPy array to Python list\n",
        "import json\n",
        "\n",
        "X_new = X_test[:3] # pretend we have 3 new digit images to classify\n",
        "request_json = json.dumps({ # is a string\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_new.tolist(),\n",
        "})"
      ],
      "metadata": {
        "id": "ZNaiVSTkE3_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gRPC API"
      ],
      "metadata": {
        "id": "HWhQ8HCqFNtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = \"serving_default\"\n",
        "input_name = model.input_names[0] # == \"flatten_input\"\n",
        "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
      ],
      "metadata": {
        "id": "KcxNzt3nHfu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grpcio library\n",
        "import grpc\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "\n",
        "channel = grpc.insecure_channel('localhost:8500')\n",
        "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "response = predict_service.Predict(request, timeout=10.0)"
      ],
      "metadata": {
        "id": "HwyvbZTBHz85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model version and export a SavedModel to directory\n",
        "model = [ ... ]\n",
        "\n",
        "model_version = \"0002\"\n",
        "model_path = Path(model_name) / model_version\n",
        "model.save(model_path, save_format=\"tf\")"
      ],
      "metadata": {
        "id": "HsPl6cNfIzbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Cloud Storage bucket to store saved models\n",
        "from google.cloud import storage\n",
        "\n",
        "project_id = \"my_project\" # change this to your project ID\n",
        "bucket_name = \"my_bucket\" # change this to a unique bucket name\n",
        "location = \"us-central1\"\n",
        "\n",
        "storage_client = storage.Client(project=project_id)\n",
        "bucket = storage_client.create_bucket(bucket_name, location=location)"
      ],
      "metadata": {
        "id": "aXQL4F5lyXBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_directory(bucket, dirpath):\n",
        "  dirpath = Path(dirpath)\n",
        "  for filepath in dirpath.glob(\"**/*\"):\n",
        "    if filepath.is_file():\n",
        "      blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix)\n",
        "      blob.upload_from_filename(filepath)\n",
        "\n",
        "upload_directory(bucket, \"my_mnist_model\")"
      ],
      "metadata": {
        "id": "HTSpdIKyzm8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "server_image = ...\n",
        "\n",
        "aiplatform.init(project=project_id, location=location)\n",
        "mnist_model = aiplatform.Model.upload(\n",
        "    display_name=\"mnist\",\n",
        "    artifact_uri=f\"gs://{bucket_name}/my_mnist_model/0001\",\n",
        "    serving_container_image_uri=server_image\n",
        ")"
      ],
      "metadata": {
        "id": "NaHHVqif0j5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy the model\n",
        "endpoint = aiplatform.Endpoint.create(display_name=\"mnist_endpoint\")\n",
        "\n",
        "endpoint.deploy(\n",
        "    mnist_model,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=5,\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
        "    accelerator_count=1\n",
        ")"
      ],
      "metadata": {
        "id": "sFu8q61rz8oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = endpoint.predict(instances=X_new.tolist())"
      ],
      "metadata": {
        "id": "bXD8mCmW1ji2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.round(response.predictions, 2)"
      ],
      "metadata": {
        "id": "RNUY3bQz28aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint.undeploy_all() # undeploy all models from the endpoint\n",
        "endpoint.delete()"
      ],
      "metadata": {
        "id": "yz0oUgJh2_qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Prediction Jobs on Vertex AI"
      ],
      "metadata": {
        "id": "dYgr9lxT3ryS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_path = Path(\"my_mnist_batch\")\n",
        "batch_path.mkdir(exist_ok=True)\n",
        "with open(batch_path / \"my_mnist_batch_jsonl\", \"w\") as jsonl_file: # jsonl stands for JSON Lines\n",
        "  for image in X_test[:100].tolist()\n",
        "  jsonl_file.write(json.dumps(image))\n",
        "  jsonl_file.write(\"\\n\")\n",
        "\n",
        "upload_directory(bucket, batch_path)"
      ],
      "metadata": {
        "id": "kiv86jFF3vE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.aiplatform_v1.types import batch_prediction_job\n",
        "\n",
        "# Prediction job\n",
        "batch_prediction_job  = mnist_model.batch_predict(\n",
        "    job_siplay_name=\"my_batch_prediction_job\",\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    starting_replica_count=1,\n",
        "    max_replica_count=5,\n",
        "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
        "    accelerator_count=1,\n",
        "    gcs_source=[f\"gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl\"],\n",
        "    gcs_destnation_prefix=f\"gs://{bucket_name}/my_mnist_predictions/\",\n",
        "    sync=True # set to False if you don't want to wait for completion\n",
        ")"
      ],
      "metadata": {
        "id": "AN4VBfYe6gQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_probas = []\n",
        "for blob in batch_prediction_job.iter_outputs():\n",
        "  if \"prediction.results\" in blob.name:\n",
        "    for line in blob.download_as_text().splitlines():\n",
        "      y_proba = json.loads(line)[\"prediction\"]\n",
        "      y_probas.append(y_proba)"
      ],
      "metadata": {
        "id": "4X1zRXcN7CcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the accuracy\n",
        "y_pred = np.argmax(y_probas, axis=1)\n",
        "accuracy = np.sum(y_pred == y_test[:100]) / 100"
      ],
      "metadata": {
        "id": "-y1xz7o48Iow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the model, directories, batch_prediction_job and bucket\n",
        "for prefix in [\"my_mnist_model/\", \"my_mnist_batch/\", \"my_mnist_predictions/\"]:\n",
        "  blobs = bucket.list_blobs(prefix=prefix)\n",
        "  for blob in blobs:\n",
        "    blob.delete()\n",
        "\n",
        "bucket.delete() # if the bucket is empty\n",
        "batch_prediction_job.delete()"
      ],
      "metadata": {
        "id": "P21tnnYZ807q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying the model to Mobile"
      ],
      "metadata": {
        "id": "dPcktYoYwySf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert a SavedModel to a FlatBuffer and save it to a .tflite file\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\n",
        "tflite_model = converter.convert()\n",
        "with open(\"my_converted_savedmodel.tflite\", \"wb\") as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "puc6AIiewt-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GPUs"
      ],
      "metadata": {
        "id": "_7A85tabxCJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if TF sees your GPUs\n",
        "import tensorflow as tf\n",
        "\n",
        "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "physical_gpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyGNOM0S2vjW",
        "outputId": "9823046f-2322-421d-dc15-0cb5887e86a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set RAM usage limit for tensorflow\n",
        "for gpu in physical_gpus:\n",
        "  tf.config.set_logical_device_configuration(\n",
        "      gpu,\n",
        "      [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
        "  )"
      ],
      "metadata": {
        "id": "6t-_wMqs27Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tell TF to grab memory only when it needs it\n",
        "for gpu in physical_gpus:\n",
        "  tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "h5XDJMpd4Eda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split one GPU into two logical devices\n",
        "tf.config.set_logical_device_configuration(\n",
        "    physical_gpus[0],\n",
        "    [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
        "     tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
        ")"
      ],
      "metadata": {
        "id": "UshB9uu54R80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.Variable([1., 2., 3.])\n",
        "a.device # .device tells you which device the variable was placed on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y7gc-ER_5oXS",
        "outputId": "95a20ec9-0ed6-461c-ab92-49b5fbb83fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/job:localhost/replica:0/task:0/device:CPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Place operation on a different device\n",
        "with tf.device(\"/gpu:0\"):\n",
        "  c = tf.Variable([1., 2., 3.])\n",
        "\n",
        "c.device # I don't have a GPU so it will be set to CPU anyways"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pKtP7F3J6Pyz",
        "outputId": "36071da7-27ec-46cf-80d5-473948974273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/job:localhost/replica:0/task:0/device:CPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training at Scale Using the Distribution Strategies API"
      ],
      "metadata": {
        "id": "X2Ox0tlXmzIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MirrorStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "  model = tf.keras.Sequentrial([...]) # Create a Keras model normally\n",
        "  model.compile([...]) # compile the model normally\n",
        "\n",
        "batch_size = 100 # preferably divisible by the numbre of replicas\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "oYC8nUzcm2ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or load a model\n",
        "with strategy.scope():\n",
        "  model = tf.keras.models.load_model(\"my_mirrored_model\")"
      ],
      "metadata": {
        "id": "qIUM4WnonS71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define subset of GPUs to use\n",
        "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])"
      ],
      "metadata": {
        "id": "dj1sxrnooKXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use data parallelism\n",
        "strategy = tf.distribute.experimental.CentralStorageStrategy()"
      ],
      "metadata": {
        "id": "dyYb2dpSof2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a model on a cluster\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "\n",
        "strategy = tf.distribute.MultiWorkerMirroredStrategy() # at the start\n",
        "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
        "print(f\"Starting task {resolver.task_type} #{resolver.task_id}\")\n",
        "[...] # load and split a dataset\n",
        "\n",
        "with strategy.scope():\n",
        "  model = tf.keras.Sequential([...]) # build the Keras model\n",
        "  model.compile([...]) # compile the model\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n",
        "\n",
        "if resolver.task_id == 0: # the chief saves the model to the right location\n",
        "  model.save(\"my_mnist_multiworker_model\", save_format=\"tf\")\n",
        "else:\n",
        "  tmpdir = tempfile.mkdtemp() # other workers save to a temporary directory\n",
        "  model.save(tmpdir, save_format=\"tf\")\n",
        "  tf.io.gfile.rmtree(tmpdir) # and we can delete this directory at the end"
      ],
      "metadata": {
        "id": "pDAWB0S4qmKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
        "    communication_options=tf.distribute.experimental.CommunicationOptions(\n",
        "        implementation=tf.distribute.experimental.CollectiveCommunication.NCCL))"
      ],
      "metadata": {
        "id": "oIopUssXrewx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TPU strategy\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "T64FaH4FsWzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Large Training Jobs on Vertex AI"
      ],
      "metadata": {
        "id": "eLrNhdtVfQNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "[...] # other imports, create MultiWorkerMirroredStrategy, and resolver\n",
        "\n",
        "if resolver.task_type == \"chief\":\n",
        "  model_dir = os.getenv(\"AIP_MODEL_DIR\") # paths provided by Vertex AI\n",
        "  tensorboard_log_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\n",
        "  checkpoint_dir = os.getenv(\"AIP_CHECKPOINT_DIR\")\n",
        "else:\n",
        "  tmp_dir = Path(tempfile.mkdtemp()) # other workers use temporary dirs\n",
        "  model_dir = tmp_dir / \"model\"\n",
        "  tensorboard_log_dir = tmp_dir / \"logs\"\n",
        "  checkpoint_dir = tmp_dir / \"ckpt\"\n",
        "\n",
        "callbacks = [tf.keras.callbacks.TensorBoard(tensorboard_log_dir),\n",
        "             tf.keras.callbacks.ModelCheckpoint(checkpoint_dir)]\n",
        "[...] # build and compile using the strategy scope, just like earlier\n",
        "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epohcs=10,\n",
        "          callbacks=callbacks)\n",
        "model.save(model_dir, save_format=\"tf\")"
      ],
      "metadata": {
        "id": "XV_d-gY0fUXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom training job on Vertex AI\n",
        "custom_training_job = aiplatform.CustomTrainingJob(\n",
        "    display_name=\"my_custom_training_job\",\n",
        "    script_path=\"my_vertex_ai_training_task.py\",\n",
        "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
        "    model_serving_container_image_uri=server_image,\n",
        "    requirements=[\"gcsfs==2022.3.0\"], # not needed this is just an example\n",
        "    staging_bucket=f\"gs://{bucket_name}/staging\"\n",
        ")"
      ],
      "metadata": {
        "id": "V3q2dfoVgk0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run it on 2 workers, each with 2 GPUs\n",
        "mnist_model2 = custom_training_job.run(\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    replica_count=2,\n",
        "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
        "    accelerator_count=2\n",
        ")"
      ],
      "metadata": {
        "id": "1d0KF1kBhDW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning on Vertex AI"
      ],
      "metadata": {
        "id": "AAczDLCQhxqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--n_hidden\", type=int, default=2)\n",
        "parser.add_argument(\"--n_neurons\", type=int, default=256)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-2)\n",
        "parser.add_argument(\"--optimizer\", default=\"adam\")\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "HP07A5G_h0E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_model(args):\n",
        "  with tf.distribute.MirroredStrategy().scope():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n",
        "    for _ in range(args.n_hidden):\n",
        "      model.add(tf.keras.layers.Dense(args.n_neurons, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "    opt = tf.keras.optimizers.get(args.optimizer)\n",
        "    opt.learning_rate = args.learning_rate\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "[...] # load dataset\n",
        "model = build_model(args)\n",
        "history = model.fit([...])"
      ],
      "metadata": {
        "id": "1ajRQJXeiF9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Report model's performance back to Vertex AI\n",
        "import hypertune\n",
        "\n",
        "hypertune = hypertune.HyperTune()\n",
        "hypertune.report_hyperparameter_tuning_metric(\n",
        "    hyperparameter_metric_tag=\"accuracy\", # name of the reported metric\n",
        "    metric_value=max(history.history[\"val_accuracy\"]), # metric value\n",
        "    global_step=model.optimizer.iterations.numpy()\n",
        "  )"
      ],
      "metadata": {
        "id": "026WhJRxi_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_job = aiplatform.CustomJob.from_local_script(\n",
        "    display_name=\"my_search_trial_job\",\n",
        "    script_path=\"my_vertex_ai_trial.py\", # path to your training script\n",
        "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
        "    staging_bucket=f\"gs://{bucket_name}/staging\",\n",
        "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
        "    accelerator_count=2, # in this example, each trial will have 2 GPUs\n",
        ")"
      ],
      "metadata": {
        "id": "0hYcM5ETjdJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run hyperparameter tuning job\n",
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "hp_job = aiplatform.HyperparameterTuninigJob(\n",
        "    display_name=\"my_hp_search_job\",\n",
        "    custom_job=trial_job,\n",
        "    metric_spec={\"accuracy\": \"maximize\"},\n",
        "    parameter_spec={\n",
        "        \"learning_rate\": hpt.DoubleParameterSpec(min=1e-3, max=10, scale=\"log\"),\n",
        "        \"n_neurons\": hpt.DoubleParameterSpec(min=1, max=300, scale=\"linear\"),\n",
        "        \"n_hidden\": hpt.DoubleParameterSpec(min=1, max=10, scale=\"linear\"),\n",
        "        \"optimizer\": hpt.CategoricalParameterSpec([\"gsd\", \"adam\"])\n",
        "    },\n",
        "    max_trial_count=100,\n",
        "    parallel_trial_count=20\n",
        ")\n",
        "\n",
        "hp_job.run()"
      ],
      "metadata": {
        "id": "sngrZ9aij5v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find best trial\n",
        "\n",
        "def get_final_metric(trial, metric_id):\n",
        "  for metric in trial.final_measurement.metrics:\n",
        "    if metric.metric_id == metric_id:\n",
        "      return metric.value\n",
        "\n",
        "trials = hp_job.trials\n",
        "trial_accuracies = [get_final_metric(trial, \"accuracy\") for trial in trials]\n",
        "best_trial = trials[np.argmax(trial_accuracies)]\n",
        "\n",
        "max(trial_accuracies), best_trial.id"
      ],
      "metadata": {
        "id": "VbiVLvLWkk-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}