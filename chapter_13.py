# -*- coding: utf-8 -*-
"""Chapter_13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18hOvYuLFyqwUVF8oyvcixkySr_t72DK2
"""

import numpy as np

arr = np.random.randint(30, size=(5))
set(arr)

"""# The tf.data API"""

# Create a dataset from a simple data tensor using tf.data.Dataset.from_tensor_slices()
import tensorflow as tf
X = tf.range(11) # any data tensor
dataset = tf.data.Dataset.from_tensor_slices(X)
dataset

# Iterate through items
for item in dataset:
  print(item)

# Do it with tuples and dictionaries etc, (name / tensor pairs)
X_nested = {"a": ([1, 2, 3], [4, 5, 6]), "b": [7, 8, 9]}
dataset = tf.data.Dataset.from_tensor_slices(X_nested)
for item in dataset:
  print(item)

# Transformations with tf.data (creates new dataset each transformation)
dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))
dataset = dataset.repeat(3).batch(7)
for item in dataset:
  print(item)

# You can use .map()
dataset = dataset.map(lambda x: x + 3)
for item in dataset:
  print(item)

# You can use .filter()
dataset = dataset.filter(lambda x: tf.reduce_sum(X) > 50)
for item in dataset:
  print(item)

# Use take() to check few items
for item in dataset.take(2):
  print(item)

# Shuffle
dataset = tf.data.Dataset.range(10).repeat(2)
dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)
for item in dataset:
  print(item)

# Shuffle filepaths
path_example = ["data/spam_data*.csv", "data/spam_data*.csv"] # simple list with data paths

filepath_dataset = tf.data.Dataset.list_files("data/spam_data*.csv", seed=42) # Shuffles the filepaths
next(iter(filepath_dataset))

n_readers = 2
dataset = filepath_dataset.interleave( # Read n_readers files at a time and interleave their lines.
    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
    cycle_length=n_readers)
dataset

for line in dataset.take(2):
  print(line)

# Preprocessing the Date (not the spam data)
X_mean, X_std = [1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 3, 34, 5, 6, 7, 8]
n_inputs = 8

def parse_csv_line(line):
  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]
  fields = tf.io.decode_csv(line, record_defaults=defs)
  return tf.stack(fields[:-1]), tf.stack(fields[-1:])

def preprocess(line):
  x, y = parse_csv_line(line)
  return (x - X_mean) / X_std, y

[0.] * 8 + [tf.constant([], dtype=tf.float32)]

preprocess(b'4.2124, 44.0, 5.3232, 0.9178, 846.0, 2.3343, 37.42, -123.34, 2.')

# Put it all together
def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,
                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,
                       batch_size=32):
  dataset = tf.data.Dataset.list_files(filepaths, seed=seed)

  dataset = dataset.interleave(
      lambda filepath: tf.data.TextLineDataset(filepaths).skip(1),
      cycle_length=n_readers, num_parallel_calls=n_parse_threads)

  dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)

  dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)

  return dataset.batch(batch_size).prefetch(1)

# Now let's create a train set
train_set = csv_reader_dataset(path_example)

# Fit the train dataset into the model
model = tf.keras.Sequential([
    tf.keras.layers.Input((8,)),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

# model.compile(loss="mse", optimizer="sgd")
# model.fit(train_set, epochs=5)

# Use other methods as well
# test_mse = model.evaluate(test_set)
# new_set = test_set.take(3) # pretend we have 3 new samples
# y_pred = model.predict(new_set) # or you could just pass a NumPy array

# Create training loop as a TF function
@tf.function
def train_one_epoch(model, optimizer, loss_fn, train_set):
  for X_batch, y_batch in train_set:
    with tf.GradientTape() as tape:
      y_pred = model(X_batch)
      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
      loss = tf.add_n([main_loss] + model.losses)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

n_epochs = 1
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
loss_fn = tf.keras.losses.MeanSquaredError()
# for epoch in range(n_epochs):
  # print("\rEpoch {}/{}".format(epoch + 1, n_epochs), end="")
  # train_one_epoch(model, optimizer, loss_fn, train_set)

"""## TFRecord Format
Preferred format for storing large amounts of data and reading it efficiently
"""

# Create TFRecord file with tf.io.TFRecordWriter class
with tf.io.TFRecordWriter("my_tfr_data.tfrecord") as f:
  f.write(b"This is the first record") # b means byte string
  f.write(b"And this is the second record")

# Use tf.data.TFRecordDataset to read TFRecord files
filepaths = ["my_tfr_data.tfrecord"]
dataset = tf.data.TFRecordDataset(filepaths)
for item in dataset:
  print(item)

# Create a compressed TFRecord file by setting the options argument
options = tf.io.TFRecordOptions(compression_type="GZIP")
with tf.io.TFRecordWriter("my_compressed_tfr.tfrecord", options) as f:
  f.write(b"This is the first record")
  f.write(b"And this is the second record")

# Read the compressed TFRecord file (you need to specify the compressioni type)
dataset_ziped = tf.data.TFRecordDataset(["my_compressed_tfr.tfrecord"],
                                  compression_type="GZIP")
for item in dataset_ziped:
  print(item)

# Compare the sizes
import os
print(f'Not-Ziped: {os.path.getsize("my_tfr_data.tfrecord")} | Ziped: {os.path.getsize("my_compressed_tfr.tfrecord")}')

"""### Protocol Buffers
Serialized protocol buffers (also protobufs) are usually consist in TFRecord files. It is portable, extensible, and efficient open source *binary format* developed at Google.

**Definition of ptorobufs looks like this:** \
`syntax = "proto3" # Version of protobuf format \
 message Person { # Person object \
    string name = 1; # Dtype name = value \
    int32 id = 2; \
    repeated string email = 3;} # Zero or more string email fields`
"""

# Create an example protobuf
from tensorflow.train import BytesList, FloatList, Int64List
from tensorflow.train import Feature, Features, Example

person_example = Example(
    features=Features(
        feature={
            "name": Feature(bytes_list=BytesList(value=[b"Alice"])),
            "id": Feature(int64_list=Int64List(value=[123])),
            "emails": Feature(bytes_list=BytesList(value=[b"info@gm.com",
                                                           b"info@ab.com"]))
        }
    )
)

# Write this example protobuf as a .tfrecord file
with tf.io.TFRecordWriter("my_example_protobuf.tfrecord") as f:
  for _ in range(5):
    f.write(person_example.SerializeToString()) # Serialize it

# Load the serialized Example protobuf
feature_description = {
    "name": tf.io.FixedLenFeature([], tf.string, default_value=""),
    "id": tf.io.FixedLenFeature([], tf.int64, default_value=0),
    "emails": tf.io.VarLenFeature(tf.string)
}

def parse(serialized_example):
  return tf.io.parse_single_example(serialized_example, feature_description)

dataset = tf.data.TFRecordDataset(["my_example_protobuf.tfrecord"]).map(parse)
for parsed_example in dataset:
  print(parsed_example)

parsed_example["emails"].values

tf.sparse.to_dense(parsed_example["emails"], default_value=b"")

# Parse examples batch by batch instead of one by one
def parse(serialized_example):
  return tf.io.parse_example(serialized_example, feature_description) # *

dataset = tf.data.TFRecordDataset(["my_example_protobuf.tfrecord"]).batch(2).map(parse)
for parsed_example in dataset:
  print(parsed_example) # two examples at a time

# For Lists of Lists use the SequenceExample Protobuf

"""## Keras Preprocessing Layers"""

import numpy as np
X_train, y_train = np.random.randn(10, 10), np.random.randint(0, 2, (10, 1))
y_train

# The normalization layer (covered in Chapter 10)
# norm_layer = tf.keras.layers.Normalization()
# model = tf.keras.Sequential([
#     norm_layer,
#     tf.keras.layers.Dense(1)
# ])
# model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(learning_rate=0.002))
# norm_layer.adapt(X_train) # computes the mean and variance of every feature
# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)

# Or use normalization layer separately
norm_layer = tf.keras.layers.Normalization()
norm_layer.adapt(X_train)
X_train_scaled = norm_layer(X_train)

X_train[0], X_train_scaled[0]

# Now we can train the model
model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, activation="sigmoid")])
model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(learning_rate=0.002))
history = model.fit(X_train_scaled, y_train, epochs=5)

# Now the model expects normalized data as input, but it has no norm_layers, let's fix that.
final_model = tf.keras.Sequential([norm_layer, model])
X_new = X_train[4]
y_pred = final_model(X_new)
y_pred

# Use map to normalize
# dataset = dataset.map(lambda X, y: (norm_layer(X), y))

# Create custom normalization layer
import numpy as np

class MyNormalization(tf.keras.layers.Layer):
  def adapt(self, X):
    self.mean_ = np.mean(X, axis=0, keepdims=True)
    self.std_ = np.std(X, axis=0, keepdims=True)

  def call(self, inputs):
    eps = tf.keras.backend.epsilon() # a small smoothing term
    return (inputs - self.mean_) / (self.std_ + eps)

norm = MyNormalization()
norm.adapt(X_train)
norm(X_train)[0]

# Discretization Layer (categorizes the data)
age = tf.constant([[10.], [93.], [57.], [18.], [37.], [6.]])
discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])
age_categories = discretize_layer(age)
age_categories # map three categories (less than 18, 18 to 50, 50+)

# Let Discretization Layer find categories itself with num_bins parameter
age = tf.constant([[10.], [93.], [57.], [18.], [37.], [6.]])
discretize_layer = tf.keras.layers.Discretization(num_bins=3)
discretize_layer.adapt(age) # But first call the .adapt() method
age_categories = discretize_layer(age)
age_categories

"""### The CategoryEncoding Layer"""

onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)
onehot_layer(age_categories)

# Multi-hot encoding
two_age_categories = np.array([[0, 2], [0, 1], [2, 2]])
onehot_layer(two_age_categories)

onehot_count_layer = tf.keras.layers.CategoryEncoding(num_tokens=3, output_mode="count") # Outputs count of each element
two_age_categories = np.array([[0, 2], [0, 1], [2, 2]])
onehot_count_layer(two_age_categories)

# [0, 2] and [2, 0] both outputs [1, 0, 1], but the indecies are different
# In the [2, 0] 0 has index of 1 and 2 has index of 0.
onehot_layer(np.array([[0, 2], [2, 0]]))

# To avoid this encode each feature separately and concatenate the outputs (per-feature one-hot encoding)
onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)
two_age_categories, onehot_layer(two_age_categories + [0, 3])

int_lookup = tf.keras.layers.IntegerLookup(output_mode="one_hot")
int_lookup.adapt(age)
int_lookup(tf.constant([[19.], [37.], [6.]]))

dict(enumerate(int_lookup.get_vocabulary())) # List the categories

# What about strings? The StringLookup Layer
cities = ["Auckland", "Paris", "Paris", "San Francisco"]
str_lookup_layer = tf.keras.layers.StringLookup()
str_lookup_layer.adapt(cities)
str_lookup_layer([["Paris"], ["Auckland"], ["Auckland"], ["Montreal"], ["San Francisco"]])

dict(enumerate(str_lookup_layer.get_vocabulary())) # List the categories

# Use "one-hot" in output_mode parameter to get a vector for each category
str_lookup_layer_o_h = tf.keras.layers.StringLookup(output_mode="one_hot")
str_lookup_layer_o_h.adapt(cities)
str_lookup_layer_o_h([["Paris"], ["Auckland"], ["Auckland"], ["Montreal"], ["San Francisco"]])

# To avoid setting unknown values to 0 use OOV(Out-Of-Vocabulary) parameter.
# It creates n indecies for unknown values. It helps the model distinguish unknown categories.
str_lookup_layer_oov = tf.keras.layers.StringLookup(num_oov_indices=5)
str_lookup_layer_oov.adapt(cities)
str_lookup_layer_oov([["Paris"], ["Montreal"], ["Montreal"], ["Unknown City"], ["UNK"], ["San Francisco"]])

dict(enumerate(str_lookup_layer_oov.get_vocabulary()))

"""### The Hashing Layer

Doesn't need `adapt()` method
"""

hashing_layer = tf.keras.layers.Hashing(num_bins=10)
hashing_layer([["Paris"], ["Auckland"], ["Montreal"], ["San Francisco"]])

"""## Encoding using Embeddings

and Representation Learning
"""

tf.random.set_seed(42)
embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)
embedding_layer(np.array([2, 4, 2]))

# What about categorical text? Chain StringLookup and Embedding layers.
tf.random.set_seed(42)
moods = ["Good", "Bad", "Appropriate", "Ok"]
str_lookup_layer = tf.keras.layers.StringLookup()
str_lookup_layer.adapt(moods)
lookup_and_embed = tf.keras.Sequential([
    str_lookup_layer,
    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(), output_dim=2)
])

lookup_and_embed(np.array(["Ok", "Badly"]))

str_lookup_layer.vocabulary_size()

# Let's create a model with text categorical embedding
tf.random.set_seed(42)
np.random.seed(42)
X_train_num = np.random.rand(10_000, 8)
X_train_cat = np.random.choice(moods, size=10_000).astype(object)
y_train = np.random.rand(10_000, 1)
X_valid_num = np.random.rand(2_000, 8)
X_valid_cat = np.random.choice(moods, size=2_000).astype(object)
y_valid = np.random.rand(2_000, 1)

X_train_cat[:5], len(X_train_cat)

# Inputs (num and cat)
num_input = tf.keras.layers.Input(shape=[8], name="num")
cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name="cat")
# Embedding layer
cat_embeddings = lookup_and_embed(cat_input)
# Concatenate layer
encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])
# Output
outputs = tf.keras.layers.Dense(1)(encoded_inputs)
# Build the model
model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])
model.compile(loss="mse", optimizer="sgd")
history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,
                    validation_data=((X_valid_num, X_valid_cat), y_valid))

import tensorflow as tf
# TextVectorization for basic text preprocessing
train_data = ["To be", "!(to be)", "That's the question", "Bla bla bla ble ble ble"]
text_vec_layer = tf.keras.layers.TextVectorization() # You can set standardize=None to preserve punctuation
text_vec_layer.adapt(train_data)
text_vec_layer(["Salza et picante", "Question: be or be?"])

text_vec_layer.get_vocabulary()

# Use output_mode=tf_idf to get weighted embedding (more frequent words are downweighted and vice versa)
text_vec_layer = tf.keras.layers.TextVectorization(output_mode="tf_idf")
text_vec_layer.adapt(train_data)
text_vec_layer(["Salsa et picante", "Ble ble, good"])

text_vec_layer.get_vocabulary()

# Use pretrained Language Model components
import tensorflow_hub as hub
hub_layer = hub.KerasLayer("https://www.kaggle.com/models/google/nnlm/TensorFlow2/en-dim50/1")
sentence_embeddings = hub_layer(tf.constant(["Salza et picante", "Bla bla bla ble ble ble"]))
sentence_embeddings.numpy().round(2)

"""### Image Preprocessing Layers"""

from sklearn.datasets import load_sample_images

images = load_sample_images()["images"]
crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)
cropped_images = crop_image_layer(images)

import matplotlib.pyplot as plt

plt.imshow(images[0])
plt.show()

# Let's experiment
random_rotation = tf.keras.layers.RandomRotation(0.23, fill_mode='nearest', data_format='channels_first')
random_zoom = tf.keras.layers.RandomZoom(1, fill_mode='nearest', data_format='channels_last')
random_contrast = tf.keras.layers.RandomContrast(1, value_range=(0, 50))

change_photo = tf.keras.Sequential([random_rotation, random_zoom, random_contrast])

plt.imshow(change_photo(images[0]))

# Let's download MNIST with tfds
import tensorflow_datasets as tfds
datasets = tfds.load(name="mnist")
mnist_train, mnist_test = datasets["train"], datasets["test"]

# Batching, prefetching, and shuffling or use .load(as_supervised=True) parameter
for batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):
  images = batch["image"]
  labels = batch["label"]

# To split the data (90% for train, 10% for validation, 100% for test)
train_set, val_set, test_set = tfds.load(name="mnist", split=["train[:90%]", "train[90:]", "test"])

"""# Exercises

### 9
"""

import tensorflow as tf

(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

X_valid, X_train = X_train_full[:7500], X_train_full[7500:]
y_valid, y_train = y_train_full[:7500], y_train_full[7500:]

train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_set.shuffle(len(X_train))
val_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))
test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))

# Serialize
from tensorflow.train import BytesList, FloatList, Int64List
from tensorflow.train import Feature, Features, Example

def serialization(image, label):
  image_data = tf.io.serialize_tensor(image)

  return Example(
        features=Features(
            feature={
                "image": Feature(bytes_list=BytesList(value=[image_data.numpy()])),
                "label": Feature(int64_list=Int64List(value=[label])),
            }))

for image, label in val_set.take(1):
  serialization(image, label)

# Train the data
normalization = tf.keras.layers.Normalization()


sample_image_batches = train_set.take(100).map(lambda image, label: image)
sample_images = np.concatenate(list(sample_image_batches),
                               axis=0).astype(np.float32)
normalization.adapt(sample_images)

model = tf.keras.Sequential([
    tf.keras.layers.Input((28, 28)),
    normalization,
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(10, activation="softmax"),
])

model.compile(loss="sparse_categorical_crossentropy",
              optimizer="nadam", metrics=["accuracy"])

model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))

"""### 10"""

train_set, val_set, test_set = tfds.load(name="imdb_reviews", split=["train[:90%]", "train[90:]", "test"],  as_supervised=True)

X_train = [X for X, _ in train_set]
y_train = [y for _, y in train_set]

X_val = [X for X, _ in val_set]
y_val = [y for _, y in val_set]

X_test = [X for X, _ in test_set]
y_test = [y for _, y in test_set]

train_set_d = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)
val_set_d = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(64)

text_vec = tf.keras.layers.TextVectorization(max_tokens=1000, output_mode="tf_idf")
text_vec.adapt(X_train_d)

bin_clf = tf.keras.Sequential([
    text_vec,
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")])

bin_clf.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adamax(learning_rate=0.01), metrics=["accuracy"])

bin_clf.fit(train_set_d, validation_data=val_set_d, epochs=5)

bin_clf.predict(tf.constant(["I mean, movie is OK, but bad"]))

test_set_d = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(64)

y_preds = bin_clf.predict(test_set_d)

labels = (y_preds >= 0.5).astype(int)

arr = (labels == y_preds)
true_pos = sum(arr)

true_pos, len(arr) - true_pos